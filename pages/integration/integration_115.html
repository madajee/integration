<div class = "container-fluid mt-2">
    <div class="row">
        <integration-side-menu></integration-side-menu>
        <div class="col-md-10 pagecover1">
            <h6 class="text-center">INTEGRATION 115</h6>
            <h3>Mule Designer</h3>
            <p>One of the suggestion when you aspire to become a mule designer is to understand and complete the Anypoint Platform Development: Fundamentals free course and keep developing mule expertise as you deliver on mule projects. I have done this course a few times and pass the certification couple of times.</p>
            <p>In the introductory section we are introduced with concepts like Application Network, Production - Consumption based IT operating model, Restful API and Services. Earlier we use to identify machines with an application but services are now implemented to run on any machine and we have cloud providers & Kubernetes engine that provides a layer of abstraction to all the underlying complexity. We have more services available as API then ever before and also there is significant push to create API for internal services as well so that delivery teams can quickly adapt and consume API & services to generate business value. Thus it is required to create and publish assets that are discoverable and leveraged by Product Owners and Buisness Architects. We can then reap benefits of resusable nodes in the form of application network with both internal and external services and APIs. Application landscape of these services & APIs is geared with restful constraints & microservices principles, and team & service topologies are adjusted as per business need.</p>
            <p>Anypoint platform allow designers to take top down approach to designing, deploying and discovering API whereas Studio, mule runtime & core components, connectors, dataweave covers the development and implementation aspects of mule flows. Anypoint provides tools like API Designer, API Console with Mocking Service, API Portal, Exchange, API Notebook is used throughout the Design-->Simulate-->Feedback-->Validate iteration of RAML spec. API spec can then imported in mule project to scaffold flows, build and test as per the requirements, architecture and design. API Manager and Runtime Manager covers the deployment and governance aspects of the APIs. There are several policies available like client ID enforcement, Open ID connect that helps you secure the APIs. There are additional tools like Anypoint Monitoring, API Analytics and Visualizer that performs the specific function of monitor, analyze and troubleshoot within the governance framework.</p>
            <p>We start on the API design by creating RAML specs in the API Designer and validates the interface layer by using the mocking service. As we design and create many API specifications, we start to reuse types, traits, library and other features of RAML. As we iterate on design, development and unit testing tasks, we build mule applications in Studio by pulling in the API specification that are published to exchange. Mule flows have sections like source, processor and error handling and we can develop mule apps in a graphical or XML editor with the components and connectors available in the mule palette. Studio has embedded mule runtime to test the developed mule flows but it can be licensed to run on any machine. MUnit is a mule testing framework that lets us write automated tests and is fully integrated into the studio. Mule Apps are deployed with Runtime Manager and supports the following deployment options:
                <ul>
                    <li>Deployed on mule runtime with resources provisioned as AWS VM instances with cloudhub 1.0. Cloudhub 2.0 is using containers on Kubernetes clusters offering on all 2 public cloud providers.</li>
                    <li>Deployed on mule runtime with resources provisioned in customer datacenter.</li>
                    <li>Deployed as container images on anypoint runtime fabric with resources provisioned in customer datacenter.</li>
                </ul>
                API proxies are created in API Manager secure and govern the mule apps through policy framework and gateway runtime included as part of mule runtime. API Manager uses auto discovery feature to connect proxy and mule app but when we API proxy is used to manage the non-mule application, it is required to deploy proxy application on a runtime.
            </p>
            <p>Deployed mule apps are either started as a listener, scheduler or File and DB triggers. Message Source generates the mule event wrapping the mule message, variables, correlationId. It is mule event that gets mutated as it is consumed and generated by several processors in a mule flow. One can develop rich mule apps with components, connectors, dataweave language. Some of core feature of integrations are message transformation, routing, iterative processing items in a collection, transcation tracing with a unique ID, parallel calls to external APIs and services, configurable connectors to databases, saas apps & cloud services are available to muleys and mule runtime is highly performant, reliable and scabable.</p>
            <p>When we develop complex integrations, we often develop features in modules which are extensible and also can be included as dependency in the POM. Not only we can create many flows/subflows in mule, but we can also create mutiple mule configuration files in a mule application. We usually create global file to encapsulate reusable configuration elements and same file can be refenced across all other mule configuration files in an application. Another best practice to externalize the application properties in a configuration file and then use the global configuration along with property placeholders to refer them in application code.</p>
            <p>Though Application Programming Interface is a layer of abstraction to the underlying development efforts, it was SOAP protocol that standardize the way webservices are called over the internet with HTTP. Though SOAP made calls platform neutral, it was still cumbersome an REST simplifiled with following characterstics:
                <ul>
                    <li>Request and responses can be transported with simple media type like JSON instead of XML which eased the serilaization and de-seralization complexities.</li>
                    <li>REST establish uniform interface with HTTP verbs & URIs instead of WSDL that is required in SOAP webservices.</li>
                </ul>
                In mule flows, we can consume both SOAP & REST services using the SOAP & HTTP connector. When http connector is configured to consume a Restful API, the behavior is configured as a requester but when we host the Mule API for consumers, HTTP connector is configured as a listener.
            </p>
            <p>Along with muleys, I have developed flows with components like scatter gather, choice router, for each based on integration scenarios and applied patterns of message routing, splitters to meet the business needs in integration projects. Error Handling is designed by catching errors at processor, flow, application and system level by using scopes like On Error Propagate, On Error Continue, and features like Error Mapping, Try / Catch.Message transformation is the core capability which is about reading and parsing data from one format, transfrom it and write it to another format. Dataweave is the programming language developed that supports functional programming paradigm. It provides many features like literal bindings, object and array data structures; functions, modules, core function libraries that includes higher order functions like map, filter, reduce; flow controls like if / else. A DWL script can receive CSV file as input and transform into an array of complex JSON objects, or receive an XML file and write the data out to a flat file format.</p>
            <p>Often times, there are enterprise usecases like data synchronization, large file processing, API Responses with Pagination that are batch-oriented than real-time and require solutions that can process records in batches as a job. Though the usecase may require message transformation at record level, the record level processing will have to be aggregated, summarized and completion metrics is reported at job level. As we are processing a large volume, we need a solution that creates & processes several batches in parallel for a job. Mule Batch processing can be applied to these ETL like scenarios which has components like Batch Job, Batch Step, and Batch Aggregator which are designed for reliable, asynchronous processing of larger-than-memory data sets. Batch Job is executed in three phases: Load & Dispatch Phase, Process Phase and On Complete Phase. Load & Dispatch phase is implicit and it is a prep phase through which mule splits the incoming collection into batches, create persistent queues and assign the Job Instance ID. In the process phase, batches are executed in parallel and each record in the batch goes through the processors in the batch step sequentially and any aggregation logic applied on the group of records in the agreegator step. In the on complete phase, a report is generated summarizing the job result with the number of processed & failed records. We can configure batch components with configurations like maxFailedRecords, acceptPolicy, acceptExpression depending on the usecase, corner & exception scenarios, test strategy & plans and connectors used in the batch processing.</p>
        </div>
    </div>
</div>
<custom-footer authorname='Jeetan' linkurl='https://www.linkedin.com/in/jeetan-madaan-37aaa113/'></custom-footer>